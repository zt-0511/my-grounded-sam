import argparse
import os
import sys
import yaml
import json
import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm  # è¿›åº¦æ¡åº“ï¼Œå¦‚æœæ²¡æœ‰è¯· pip install tqdm

# æ·»åŠ è·¯å¾„ä¾èµ–
sys.path.append(os.path.join(os.getcwd(), "GroundingDINO"))
sys.path.append(os.path.join(os.getcwd(), "segment_anything"))

# å¯¼å…¥ RAG å¼•æ“ (å¿…é¡»ç¡®ä¿ rag_engine.py åœ¨åŒçº§ç›®å½•)
try:
    sys.path.append(os.getcwd()) # å¼ºåˆ¶æŠŠå½“å‰ç›®å½•åŠ å…¥æœç´¢è·¯å¾„
    from rag_engine import PlantRAGSystem
except ImportError as e:
    print(f"âŒ è‡´å‘½é”™è¯¯: æ— æ³•å¯¼å…¥ rag_engineã€‚")
    print(f"ğŸ” çœŸå®æŠ¥é”™ä¿¡æ¯: {e}")  # <--- è¿™è¡Œä»£ç ä¼šå‘Šè¯‰ä½ çœŸç›¸
    print(f"ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print("ğŸ’¡ æç¤º: å¦‚æœæŠ¥é”™æ˜¯ 'No module named sentence_transformers'ï¼Œè¯·è¿è¡Œ pip install sentence-transformers")
    sys.exit(1)

# Grounding DINO å¯¼å…¥
import GroundingDINO.groundingdino.datasets.transforms as T
from GroundingDINO.groundingdino.models import build_model
from GroundingDINO.groundingdino.util.slconfig import SLConfig
from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap

# SAM å¯¼å…¥
from segment_anything import (
    sam_model_registry,
    sam_hq_model_registry,
    SamPredictor
)

# ==========================================
# æ ¸å¿ƒç®—æ³•å¢å¼ºæ¨¡å— (Core Enhancement Modules)
# ==========================================

def soft_nms_pytorch(dets, box_scores, sigma=0.1, thresh=0.001):
    """
    [Stage 1 Enhancement] Soft-NMS
    sigma=0.1: ææ¸©å’ŒæŠ‘åˆ¶ï¼Œæœ€å¤§ç¨‹åº¦ä¿ç•™å¯†é›†é‡å çš„çœŸå®ç›®æ ‡
    """
    if dets.shape[0] == 0:
        return torch.tensor([]).long(), torch.tensor([])

    N = dets.shape[0]
    indexes = torch.arange(0, N, dtype=torch.long).view(N)
    dets = dets.float()
    box_scores = box_scores.float()

    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    areas = (x2 - x1) * (y2 - y1)

    for i in range(N):
        tscore = box_scores[i].clone()
        pos = i + 1
        if i != N - 1:
            maxscore, maxpos = torch.max(box_scores[pos:], dim=0)
            if tscore < maxscore:
                dets[i], dets[maxpos + pos] = dets[maxpos + pos].clone(), dets[i].clone()
                box_scores[i], box_scores[maxpos + pos] = box_scores[maxpos + pos].clone(), box_scores[i].clone()
                areas[i], areas[maxpos + pos] = areas[maxpos + pos].clone(), areas[i].clone()
                indexes[i], indexes[maxpos + pos] = indexes[maxpos + pos].clone(), indexes[i].clone()

        xx1 = torch.maximum(dets[i, 0], dets[i+1:, 0])
        yy1 = torch.maximum(dets[i, 1], dets[i+1:, 1])
        xx2 = torch.minimum(dets[i, 2], dets[i+1:, 2])
        yy2 = torch.minimum(dets[i, 3], dets[i+1:, 3])

        w = torch.maximum(torch.tensor(0.0), xx2 - xx1)
        h = torch.maximum(torch.tensor(0.0), yy2 - yy1)
        inter = w * h
        ovr = inter / (areas[i] + areas[i+1:] - inter)

        weight = torch.exp(-(ovr * ovr) / sigma)
        box_scores[i+1:] = box_scores[i+1:] * weight

    keep = box_scores > thresh
    return indexes[keep], box_scores[keep]


def refine_mask(mask_tensor, is_tiny_object=False):
    """
    [Stage 2 Enhancement] è‡ªé€‚åº”å½¢æ€å­¦ä¿®å¤
    - å¾®å°è™«å®³: é—­è¿ç®—è¿æ¥ (ç¦æ­¢è…èš€)
    - æ¡çº¹ç—…å®³: å¼€è¿ç®—åˆ‡æ–­ (ç¦æ­¢ç²˜è¿)
    """
    mask_np = (mask_tensor.cpu().numpy() * 255).astype(np.uint8)
    
    if is_tiny_object:
        # === è™«å®³ç­–ç•¥ (Tiny Mode) ===
        # ä½œç”¨ï¼šè¿æ¥æ–­è£‚çš„è™«è…¿/è§¦è§’ï¼Œä¸¥ç¦è…èš€
        kernel = np.ones((2, 2), np.uint8)
        mask_result = cv2.morphologyEx(mask_np, cv2.MORPH_CLOSE, kernel)
    else:
        # === ç—…å®³ç­–ç•¥ (Texture Mode) ===
        # é’ˆå¯¹æ¡é”ˆç—…ä¼˜åŒ–ï¼šä½¿ç”¨å¾®å°å¼€è¿ç®—åˆ‡æ–­æ¡çº¹é—´çš„ç²˜è¿
        kernel = np.ones((2, 2), np.uint8)
        mask_result = cv2.morphologyEx(mask_np, cv2.MORPH_OPEN, kernel)
        
        # è¾¹ç¼˜å¹³æ»‘
        mask_result = cv2.GaussianBlur(mask_result, (3, 3), 0)
        _, mask_result = cv2.threshold(mask_result, 127, 255, cv2.THRESH_BINARY)
    
    return torch.from_numpy(mask_result > 0).bool()


def save_structured_result(output_dir, image_name, diagnosis_data, masks, boxes, scores, labels, rag_info=None):
    """
    [Stage 3 Enhancement] ç”ŸæˆåŒ…å« RAG ä¿¡æ¯çš„ç»“æ„åŒ–æŠ¥å‘Š
    """
    result = {
        "image_id": image_name,
        "ai_diagnosis": {
            "disease_name": diagnosis_data.get("disease_name", "Unknown") if diagnosis_data else "Local Mode",
            "target_type": diagnosis_data.get("target_type", "Unknown") if diagnosis_data else "Unknown",
        },
        "rag_metadata": rag_info if rag_info else "RAG Not Triggered",
        "detections": []
    }
    
    for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):
        clean_label = label.split('(')[0] if '(' in label else label
        result["detections"].append({
            "id": i + 1,
            "label": clean_label,
            "confidence": round(float(score), 4),
            "bbox": box.tolist(),
        })
        
    json_path = os.path.join(output_dir, "analysis_result.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=4)


# ==========================================
# åŸºç¡€è¾…åŠ©å‡½æ•°
# ==========================================

# def load_image(image_path):
#     image_pil = Image.open(image_path).convert("RGB")
#     transform = T.Compose([
#         T.RandomResize([800], max_size=1333),
#         T.ToTensor(),
#         T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
#     ])
#     image, _ = transform(image_pil, None)
#     return image_pil, image


# def load_model(model_config_path, model_checkpoint_path, bert_base_uncased_path, device):
#     args = SLConfig.fromfile(model_config_path)
#     args.device = device
#     if bert_base_uncased_path:
#         args.bert_base_uncased_path = bert_base_uncased_path
#     model = build_model(args)
#     checkpoint = torch.load(model_checkpoint_path, map_location="cpu")
#     model.load_state_dict(clean_state_dict(checkpoint["model"]), strict=False)
#     model.eval()
#     return model


# def filter_boxes_by_area(boxes, logits, max_area_threshold=0.30):
#     if boxes.shape[0] == 0:
#         return boxes, logits
#     areas = boxes[:, 2] * boxes[:, 3]
#     keep_mask = areas < max_area_threshold
#     return boxes[keep_mask], logits[keep_mask]


# def get_grounding_output(model, image, caption, box_threshold, text_threshold, max_area_threshold=0.30, with_logits=True, device="cpu"):
#     caption = caption.lower().strip()
#     if not caption.endswith("."):
#         caption += "."
    
#     model = model.to(device)
#     image = image.to(device)
    
#     with torch.no_grad():
#         outputs = model(image[None], captions=[caption])
    
#     logits = outputs["pred_logits"].cpu().sigmoid()[0]
#     boxes = outputs["pred_boxes"].cpu()[0]

#     filt_mask = logits.max(dim=1)[0] > box_threshold
#     logits_filt = logits[filt_mask]
#     boxes_filt = boxes[filt_mask]

#     boxes_filt, logits_filt = filter_boxes_by_area(boxes_filt, logits_filt, max_area_threshold)

#     scores = logits_filt.max(dim=1)[0]
#     if len(scores) > 0:
#         # Soft-NMS è°ƒç”¨
#         keep_indices, updated_scores = soft_nms_pytorch(boxes_filt, scores, sigma=0.1, thresh=box_threshold)
#         boxes_filt = boxes_filt[keep_indices]
#         logits_filt = logits_filt[keep_indices]

#     tokenlizer = model.tokenizer
#     tokenized = tokenlizer(caption)
#     pred_phrases = []
    
#     for logit, box in zip(logits_filt, boxes_filt):
#         pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenlizer)
#         if with_logits:
#             pred_phrases.append(pred_phrase + f"({str(logit.max().item())[:4]})")
#         else:
#             pred_phrases.append(pred_phrase)
            
#     return boxes_filt, pred_phrases, logits_filt


# def show_mask(mask, ax, random_color=False):
#     if random_color:
#         color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
#     else:
#         color = np.array([30/255, 144/255, 255/255, 0.6])
#     h, w = mask.shape[-2:]
#     mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
#     ax.imshow(mask_image)


# def show_box(box, ax, label):
#     x0, y0 = box[0], box[1]
#     w, h = box[2] - box[0], box[3] - box[1]
#     ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))
#     ax.text(x0, y0, label)


# def save_mask_data(output_dir, mask_list, box_list, label_list):
#     value = 0
#     mask_img = torch.zeros(mask_list.shape[-2:])
#     for idx, mask in enumerate(mask_list):
#         mask_img[mask.cpu().numpy()[0] == True] = value + idx + 1
#     plt.figure(figsize=(10, 10))
#     plt.imshow(mask_img.numpy())
#     plt.axis('off')
#     plt.savefig(os.path.join(output_dir, 'mask.jpg'), bbox_inches="tight", dpi=300, pad_inches=0.0)
#     plt.close() # å¿…é¡»å…³é—­ï¼Œå¦åˆ™æ‰¹é‡å¤„ç†ä¼šçˆ†å†…å­˜

#     json_data = [{'value': value, 'label': 'background'}]
#     for label, box in zip(label_list, box_list):
#         value += 1
#         name, logit = label.split('(')
#         logit = logit[:-1]
#         json_data.append({
#             'value': value,
#             'label': name,
#             'logit': float(logit),
#             'box': box.numpy().tolist(),
#         })
#     with open(os.path.join(output_dir, 'mask.json'), 'w') as f:
#         json.dump(json_data, f, indent=2)


# def load_config(config_path):
#     with open(config_path, 'r', encoding='utf-8') as f:
#         return yaml.safe_load(f)


# # ==========================================
# # å•å¼ å›¾ç‰‡å¤„ç†ç®¡çº¿ (Pipeline)
# # ==========================================
# def process_single_image(input_image_path, root_output_dir, models, config, args, rag_system):
#     filename = os.path.basename(input_image_path)
#     file_stem = os.path.splitext(filename)[0]
#     current_output_dir = os.path.join(root_output_dir, file_stem)
#     os.makedirs(current_output_dir, exist_ok=True)

#     gdino_model, sam_predictor = models
#     device = args.device
    
#     # é»˜è®¤å€¼
#     current_max_area = config.get("max_area_threshold", 0.30)
#     current_box_thresh = config.get("box_threshold", 0.30)
#     text_thresh = config.get("text_threshold", 0.25)
#     text_prompt = config.get("text_prompt", "plant disease")
#     is_tiny_mode = False
#     rag_info = None
#     diagnosis = None

#     if args.use_api:
#         from multimodal_expert import get_plant_diagnosis_via_api
#         try:
#             diagnosis = get_plant_diagnosis_via_api(
#                 image_path=input_image_path,
#                 access_key_id=args.access_key_id or config.get("access_key_id"),
#                 access_key_secret=args.access_key_secret or config.get("access_key_secret"),
#                 region_id=config.get("region_id", "cn-beijing")
#             )
     
#             disease_name_en = diagnosis.get('english_name', 'Unknown')
#             visual_desc = diagnosis.get('english_prompt', '')
            

#             #level 2 å½“ä¸å‘½ä¸­æ—¶ä½¿ç”¨è¿™ä¸ªå‚æ•°
#             if "å¾®å°ä¸ªä½“" in target_type:
#                 current_max_area = config.get("max_area_threshold_pest", 0.20)
#                 current_box_thresh = 0.20 
#                 is_tiny_mode = True 
#                 print(f"      ç­–ç•¥è°ƒæ•´: [é€šç”¨è™«å®³æ¨¡å¼] (Area<{current_max_area})")
#             elif "æˆç‰‡çº¹ç†" in target_type:
#                 current_max_area = config.get("max_area_threshold_disease", 0.60)
#                 current_box_thresh = 0.25 
#                 is_tiny_mode = False
#                 print(f"      ç­–ç•¥è°ƒæ•´: [é€šç”¨ç—…å®³æ¨¡å¼] (Area<{current_max_area})")


#             # è¿‡æ»¤æ‰æ— æ„ä¹‰çš„é»˜è®¤å€¼
#             query_parts = []
            
#             # 1. åŠ å…¥è‹±æ–‡å (æƒé‡æœ€é«˜)
#             if disease_name_en and disease_name_en not in ["Unknown", "Error", "None"]:
#                 query_parts.append(disease_name_en)
            
#             # 2. åŠ å…¥è§†è§‰æè¿° (ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼Œå¢åŠ åŒ¹é…åº¦)
#             # åªæœ‰å½“æè¿°ä¸æ˜¯é»˜è®¤çš„å ä½ç¬¦æ—¶æ‰åŠ 
#             if visual_desc and "plant disease symptoms" not in visual_desc:
#                 query_parts.append(visual_desc)
            
#             # 3. åˆå¹¶æˆä¸€ä¸ªé•¿å¥å­
#             if query_parts:
#                 search_query = " ".join(query_parts)
#             else:
#                 # å…œåº•ï¼šå¦‚æœéƒ½æå–å¤±è´¥ï¼Œå°±ç”¨ä¸­æ–‡åç¢°è¿æ°”ï¼ˆè™½ç„¶å¤§æ¦‚ç‡åŒ¹é…ä¸åˆ°ï¼‰
#                 search_query = diagnosis.get('disease_name', 'Unknown')

#             print(f"   -> ğŸ” RAG å¤åˆæ£€ç´¢è¯: '{search_query}'")
            
#             # å‘èµ·æ£€ç´¢
#             rag_knowledge = rag_system.search(search_query)
            
#             if rag_knowledge:
#                 print(f"   -> ğŸ“š [RAG å‘½ä¸­] åŒ¹é…: {rag_knowledge['disease_name']}")
#                 text_prompt = rag_knowledge['grounding_prompt']
#                 current_box_thresh = rag_knowledge['thresholds']['box']
#                 current_max_area = rag_knowledge['thresholds']['area']
#                 strategy = rag_knowledge.get('refine_strategy', 'normal')
#                 is_tiny_mode = (strategy == "tiny_mode")
                
#                 rag_info = {
#                     "matched_disease": rag_knowledge['disease_name'],
#                     "strategy": strategy,
#                     "prompt_used": text_prompt
#                 }
#             else:
#                 print("   -> âš ï¸ [RAG æœªå‘½ä¸­] ä½¿ç”¨é»˜è®¤é…ç½®")
#                 text_prompt = diagnosis.get('english_prompt', text_prompt)

#         except Exception as e:
#             print(f"   âŒ API/RAG é”™è¯¯: {e}")

#     # æ£€æµ‹
#     image_pil, image = load_image(input_image_path)
#     # ä¿å­˜åŸå›¾
#     image_pil.save(os.path.join(current_output_dir, "raw_image.jpg"))

#     print(f"   -> ğŸš€ æ£€æµ‹: '{text_prompt}'")
#     boxes_filt, pred_phrases, logits_filt = get_grounding_output(
#         gdino_model, image, text_prompt, current_box_thresh, text_thresh, 
#         max_area_threshold=current_max_area, device=device
#     )

#     if boxes_filt.shape[0] == 0:
#         print("   âš ï¸ æ— ç›®æ ‡ï¼Œè·³è¿‡ã€‚")
#         return

#     # SAM åˆ†å‰²
#     image_cv = cv2.imread(input_image_path) # BGR
#     image_cv_rgb = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB) # RGB
#     sam_predictor.set_image(image_cv_rgb)

#     W, H = image_pil.size
#     for i in range(boxes_filt.size(0)):
#         boxes_filt[i] = boxes_filt[i] * torch.Tensor([W, H, W, H])
#         boxes_filt[i][:2] -= boxes_filt[i][2:] / 2
#         boxes_filt[i][2:] += boxes_filt[i][:2]

#     boxes_filt = boxes_filt.cpu()
#     transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes_filt, image_cv_rgb.shape[:2]).to(device)
    
#     masks, _, _ = sam_predictor.predict_torch(
#         point_coords=None, point_labels=None, boxes=transformed_boxes, multimask_output=False,
#     )

#     # ä¼˜åŒ– Mask
#     refined_masks = []
#     for mask in masks:
#         refined_masks.append(refine_mask(mask[0], is_tiny_object=is_tiny_mode)) 
#     masks = torch.stack(refined_masks).unsqueeze(1).to(device)


#     # ç»˜å›¾æ€»è§ˆ
#     plt.figure(figsize=(10, 10))
#     plt.imshow(image_cv_rgb)
#     for mask in masks:
#         show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)
#     for box, label in zip(boxes_filt, pred_phrases):
#         show_box(box.numpy(), plt.gca(), label)
#     plt.axis('off')
#     plt.savefig(os.path.join(current_output_dir, "grounded_sam_output.jpg"), bbox_inches="tight", dpi=300, pad_inches=0.0)
#     plt.close()

#     final_scores = logits_filt.max(dim=1)[0].cpu().numpy()
#     save_structured_result(current_output_dir, filename, diagnosis, masks, boxes_filt, final_scores, pred_phrases, rag_info)

# ==========================================
# åŸºç¡€è¾…åŠ©å‡½æ•°
# ==========================================

def load_image(image_path):
    image_pil = Image.open(image_path).convert("RGB")
    transform = T.Compose([
        T.RandomResize([800], max_size=1333),
        T.ToTensor(),
        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ])
    image, _ = transform(image_pil, None)
    return image_pil, image


def load_model(model_config_path, model_checkpoint_path, bert_base_uncased_path, device):
    args = SLConfig.fromfile(model_config_path)
    args.device = device
    if bert_base_uncased_path:
        args.bert_base_uncased_path = bert_base_uncased_path
    model = build_model(args)
    checkpoint = torch.load(model_checkpoint_path, map_location="cpu")
    model.load_state_dict(clean_state_dict(checkpoint["model"]), strict=False)
    model.eval()
    return model


def filter_boxes_by_area(boxes, logits, max_area_threshold=0.30):
    if boxes.shape[0] == 0:
        return boxes, logits
    areas = boxes[:, 2] * boxes[:, 3]
    keep_mask = areas < max_area_threshold
    return boxes[keep_mask], logits[keep_mask]


def get_grounding_output(model, image, caption, box_threshold, text_threshold, max_area_threshold=0.30, with_logits=True, device="cpu"):
    caption = caption.lower().strip()
    if not caption.endswith("."):
        caption += "."
    
    model = model.to(device)
    image = image.to(device)
    
    with torch.no_grad():
        outputs = model(image[None], captions=[caption])
    
    logits = outputs["pred_logits"].cpu().sigmoid()[0]
    boxes = outputs["pred_boxes"].cpu()[0]

    filt_mask = logits.max(dim=1)[0] > box_threshold
    logits_filt = logits[filt_mask]
    boxes_filt = boxes[filt_mask]

    # é¢ç§¯è¿‡æ»¤
    boxes_filt, logits_filt = filter_boxes_by_area(boxes_filt, logits_filt, max_area_threshold)

    scores = logits_filt.max(dim=1)[0]
    if len(scores) > 0:
        # Soft-NMS è°ƒç”¨
        keep_indices, updated_scores = soft_nms_pytorch(boxes_filt, scores, sigma=0.1, thresh=box_threshold)
        boxes_filt = boxes_filt[keep_indices]
        logits_filt = logits_filt[keep_indices]

    tokenlizer = model.tokenizer
    tokenized = tokenlizer(caption)
    pred_phrases = []
    
    for logit, box in zip(logits_filt, boxes_filt):
        pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenlizer)
        if with_logits:
            pred_phrases.append(pred_phrase + f"({str(logit.max().item())[:4]})")
        else:
            pred_phrases.append(pred_phrase)
            
    return boxes_filt, pred_phrases, logits_filt


def show_mask(mask, ax, random_color=False):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)


def show_box(box, ax, label):
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))
    ax.text(x0, y0, label)


def save_mask_data(output_dir, mask_list, box_list, label_list):
    value = 0
    mask_img = torch.zeros(mask_list.shape[-2:])
    for idx, mask in enumerate(mask_list):
        mask_img[mask.cpu().numpy()[0] == True] = value + idx + 1
    plt.figure(figsize=(10, 10))
    plt.imshow(mask_img.numpy())
    plt.axis('off')
    plt.savefig(os.path.join(output_dir, 'mask.jpg'), bbox_inches="tight", dpi=300, pad_inches=0.0)
    plt.close()

    json_data = [{'value': value, 'label': 'background'}]
    for label, box in zip(label_list, box_list):
        value += 1
        name, logit = label.split('(')
        logit = logit[:-1]
        json_data.append({
            'value': value,
            'label': name,
            'logit': float(logit),
            'box': box.numpy().tolist(),
        })
    with open(os.path.join(output_dir, 'mask.json'), 'w') as f:
        json.dump(json_data, f, indent=2)


def load_config(config_path):
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


# ==========================================
# å•å¼ å›¾ç‰‡å¤„ç†ç®¡çº¿ (Pipeline)
# ==========================================
def process_single_image(input_image_path, root_output_dir, models, config, args, rag_system):
    filename = os.path.basename(input_image_path)
    file_stem = os.path.splitext(filename)[0]
    
    current_output_dir = os.path.join(root_output_dir, file_stem)
    os.makedirs(current_output_dir, exist_ok=True)

    gdino_model, sam_predictor = models
    device = args.device
    
    # === Level 3: åŠ è½½é»˜è®¤å…œåº•å‚æ•° (Lowest Priority) ===
    current_max_area = config.get("max_area_threshold", 0.30)
    current_box_thresh = config.get("box_threshold", 0.30)
    text_thresh = config.get("text_threshold", 0.25)
    text_prompt = config.get("text_prompt", "plant disease")
    is_tiny_mode = False
    rag_info = None
    diagnosis = None

    if args.use_api:
        from multimodal_expert import get_plant_diagnosis_via_api
        try:
            diagnosis = get_plant_diagnosis_via_api(
                image_path=input_image_path,
                access_key_id=args.access_key_id or config.get("access_key_id"),
                access_key_secret=args.access_key_secret or config.get("access_key_secret"),
                region_id=config.get("region_id", "cn-beijing")
            )
            
            # æå– API è¿”å›çš„ä¿¡æ¯
            disease_name_cn = diagnosis.get('disease_name', 'Unknown')
            disease_name_en = diagnosis.get('english_name', 'Unknown')
            visual_desc = diagnosis.get('english_prompt', '')
            # [å…³é”®ä¿®å¤] å¿…é¡»åœ¨è¿™é‡Œæå– target_typeï¼ŒLevel 2 æ‰èƒ½ç”¨
            target_type = diagnosis.get("target_type", "") 
            
            print(f"   -> ğŸ¤– è¯Šæ–­ç»“æœ: {disease_name_cn} (Eng: {disease_name_en})")
            print(f"   -> âš–ï¸ ç›®æ ‡ç±»å‹: {target_type}")

            # === Level 2: æ ¹æ®ç›®æ ‡ç±»å‹åº”ç”¨é€šç”¨è§„åˆ™ (Middle Priority) ===
            # å¦‚æœ RAG æ²¡å‘½ä¸­ï¼Œè¿™å¥—å‚æ•°å°±æ˜¯ç”Ÿæ•ˆçš„â€œæœ€ä½³æ›¿è¡¥â€
            if "å¾®å°ä¸ªä½“" in target_type:
                current_max_area = config.get("max_area_threshold_pest", 0.20)
                current_box_thresh = 0.20 
                is_tiny_mode = True 
                print(f"      ç­–ç•¥è°ƒæ•´: [é€šç”¨è™«å®³æ¨¡å¼] (Area<{current_max_area})")
            elif "æˆç‰‡çº¹ç†" in target_type:
                current_max_area = config.get("max_area_threshold_disease", 0.60)
                current_box_thresh = 0.25 
                is_tiny_mode = False
                print(f"      ç­–ç•¥è°ƒæ•´: [é€šç”¨ç—…å®³æ¨¡å¼] (Area<{current_max_area})")

            # === Level 1: RAG ä¸“å®¶ç³»ç»Ÿ (Highest Priority) ===
            # æ„å»ºå¤åˆæŸ¥è¯¢è¯
            query_parts = []
            if disease_name_en and disease_name_en not in ["Unknown", "Error", "None"]:
                query_parts.append(disease_name_en)
            if visual_desc and "plant disease symptoms" not in visual_desc:
                query_parts.append(visual_desc)
            
            if query_parts:
                search_query = " ".join(query_parts)
            else:
                search_query = disease_name_cn # å…œåº•

            print(f"   -> ğŸ” RAG å¤åˆæ£€ç´¢è¯: '{search_query}'")
            
            rag_knowledge = rag_system.search(search_query)
            
            if rag_knowledge:
                print(f"   -> ğŸ“š [RAG å‘½ä¸­] ä½¿ç”¨ '{rag_knowledge['disease_name']}' ä¸“å®¶é…ç½®")
                # è¦†ç›– Prompt
                text_prompt = rag_knowledge['grounding_prompt']
                # è¦†ç›–é˜ˆå€¼ (è¿™é‡Œä¼šè¦†ç›–æ‰ Level 2 çš„è®¾ç½®)
                current_box_thresh = rag_knowledge['thresholds']['box']
                current_max_area = rag_knowledge['thresholds']['area']
                # è¦†ç›–ç­–ç•¥
                strategy = rag_knowledge.get('refine_strategy', 'normal')
                is_tiny_mode = (strategy == "tiny_mode")
                
                rag_info = {
                    "matched_disease": rag_knowledge['disease_name'],
                    "strategy": strategy,
                    "prompt_used": text_prompt
                }
            else:
                print("   -> âš ï¸ [RAG æœªå‘½ä¸­] ä¿æŒ Level 2 é€šç”¨é…ç½®")
                text_prompt = visual_desc if visual_desc else text_prompt

        except Exception as e:
            print(f"   âŒ API/RAG æµç¨‹é”™è¯¯: {e}")

    # === 2. Grounding DINO æ£€æµ‹ ===
    image_pil, image = load_image(input_image_path)
    image_pil.save(os.path.join(current_output_dir, "raw_image.jpg"))

    print(f"   -> ğŸš€ æ£€æµ‹æç¤ºè¯: '{text_prompt}' (Box>{current_box_thresh}, Area<{current_max_area})")
    boxes_filt, pred_phrases, logits_filt = get_grounding_output(
        gdino_model, image, text_prompt, current_box_thresh, text_thresh, 
        max_area_threshold=current_max_area, device=device
    )

    if boxes_filt.shape[0] == 0:
        print("   âš ï¸ æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè·³è¿‡åç»­æ­¥éª¤ã€‚")
        return

    # === 3. SAM åˆ†å‰² ===
    image_cv = cv2.imread(input_image_path)
    image_cv_rgb = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)
    sam_predictor.set_image(image_cv_rgb)

    W, H = image_pil.size
    for i in range(boxes_filt.size(0)):
        boxes_filt[i] = boxes_filt[i] * torch.Tensor([W, H, W, H])
        boxes_filt[i][:2] -= boxes_filt[i][2:] / 2
        boxes_filt[i][2:] += boxes_filt[i][:2]

    boxes_filt = boxes_filt.cpu()
    transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes_filt, image_cv_rgb.shape[:2]).to(device)
    
    masks, _, _ = sam_predictor.predict_torch(
        point_coords=None, point_labels=None, boxes=transformed_boxes, multimask_output=False,
    )

    # === 4. Mask ä¼˜åŒ– ===
    refined_masks = []
    for mask in masks:
        refined_masks.append(refine_mask(mask[0], is_tiny_object=is_tiny_mode)) 
    masks = torch.stack(refined_masks).unsqueeze(1).to(device)

    # === 5. ç»“æœä¿å­˜ ===
    
    # B. ä¿å­˜å¯è§†åŒ–å›¾
    plt.figure(figsize=(10, 10))
    plt.imshow(image_cv_rgb)
    for mask in masks:
        show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)
    for box, label in zip(boxes_filt, pred_phrases):
        show_box(box.numpy(), plt.gca(), label)
    plt.axis('off')
    plt.savefig(os.path.join(current_output_dir, "grounded_sam_output.jpg"), bbox_inches="tight", dpi=300, pad_inches=0.0)
    plt.close()

    # C. ä¿å­˜æ•°æ®
    final_scores = logits_filt.max(dim=1)[0].cpu().numpy()
    save_structured_result(current_output_dir, filename, diagnosis, masks, boxes_filt, final_scores, pred_phrases, rag_info)
    save_mask_data(current_output_dir, masks, boxes_filt, pred_phrases)
# ==========================================
# ä¸»å…¥å£
# ==========================================

# ==========================================
# ä¸»å…¥å£ (ä¿®æ­£ç‰ˆï¼šæ”¯æŒçº¯ Config å¯åŠ¨)
# ==========================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser("Grounded-SAM Batch Processing with RAG")
    parser.add_argument("--config_file", type=str, default="config.yaml")
    
    # [å…³é”®ä¿®æ”¹] å»æ‰äº† required=Trueï¼Œå…è®¸å‘½ä»¤è¡Œä¸ä¼ è¿™ä¸¤ä¸ªå‚æ•°
    parser.add_argument("--input_image", type=str, help="å¯ä»¥æ˜¯å•å¼ å›¾ç‰‡è·¯å¾„ï¼Œä¹Ÿå¯ä»¥æ˜¯æ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--output_dir", type=str)
    
    parser.add_argument("--device", type=str)
    parser.add_argument("--use_api", action="store_true")
    parser.add_argument("--access_key_id", type=str)
    parser.add_argument("--access_key_secret", type=str)

    args = parser.parse_args()
    config = load_config(args.config_file)
    
    # === å‚æ•°ä¼˜å…ˆçº§å¤„ç†é€»è¾‘ (å‘½ä»¤è¡Œ > Config > æŠ¥é”™) ===
    
    # 1. å¤„ç†è¾“å…¥è·¯å¾„
    input_path = args.input_image # å…ˆçœ‹å‘½ä»¤è¡Œ
    if input_path is None:        # å‘½ä»¤è¡Œæ²¡ä¼ ï¼Œå» Config æ‰¾
        input_path = config.get("input_image")
    
    if input_path is None:        # Config ä¹Ÿæ²¡å†™ï¼ŒæŠ¥é”™
        raise ValueError("âŒ é”™è¯¯: æœªæŒ‡å®šè¾“å…¥å›¾ç‰‡è·¯å¾„ï¼è¯·åœ¨å‘½ä»¤è¡Œä½¿ç”¨ --input_image æˆ–åœ¨ config.yaml ä¸­é…ç½® input_image")

    # 2. å¤„ç†è¾“å‡ºè·¯å¾„
    output_dir = args.output_dir
    if output_dir is None:
        output_dir = config.get("output_dir")
        
    if output_dir is None:
        raise ValueError("âŒ é”™è¯¯: æœªæŒ‡å®šè¾“å‡ºç›®å½•ï¼è¯·åœ¨å‘½ä»¤è¡Œä½¿ç”¨ --output_dir æˆ–åœ¨ config.yaml ä¸­é…ç½® output_dir")

    # 3. å¤„ç†å…¶ä»–å‚æ•°
    device = args.device if args.device else config.get("device", "cuda")
    use_api = args.use_api or config.get("use_api", False)

    # ==========================================

    # 1. åˆå§‹åŒ– RAG ç³»ç»Ÿ
    print("ğŸ“š åˆå§‹åŒ– RAG çŸ¥è¯†åº“...")
    try:
        rag_system = PlantRAGSystem("knowledge_base.json")
    except Exception as e:
        print(f"âš ï¸ RAG åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ™®é€šæ¨¡å¼è¿è¡Œ")
        rag_system = None

    # 2. åŠ è½½æ¨¡å‹
    print("â³ åŠ è½½è§†è§‰æ¨¡å‹...")
    gdino_model = load_model(config["config"], config["grounded_checkpoint"], config.get("bert_base_uncased_path"), device)
    
    use_sam_hq = config.get("use_sam_hq", False)
    if use_sam_hq:
        sam = sam_hq_model_registry[config["sam_version"]](checkpoint=config.get("sam_hq_checkpoint"))
    else:
        sam = sam_model_registry[config["sam_version"]](checkpoint=config["sam_checkpoint"])
    sam.to(device=device)
    sam_predictor = SamPredictor(sam)
    
    models = (gdino_model, sam_predictor)

    # 3. å‡†å¤‡æ–‡ä»¶åˆ—è¡¨
    image_files = []
    
    if os.path.isdir(input_path):
        print(f"ğŸ“‚ æ‰¹é‡å¤„ç†ç›®å½•: {input_path}")
        valid_exts = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')
        # å¢åŠ å¯¹å¤§å°å†™åç¼€çš„å…¼å®¹
        image_files = [os.path.join(input_path, f) for f in os.listdir(input_path) if f.lower().endswith(valid_exts)]
    else:
        print(f"ğŸ“„ å¤„ç†å•å¼ å›¾ç‰‡: {input_path}")
        image_files = [input_path]

    if len(image_files) == 0:
        print(f"âŒ é”™è¯¯: åœ¨è·¯å¾„ {input_path} ä¸‹æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶ï¼")
        sys.exit(1)

    # 4. å¼€å§‹å¾ªç¯å¤„ç†
    print(f"ğŸš€ å¼€å§‹å¤„ç† {len(image_files)} å¼ å›¾ç‰‡...")
    
    # é‡æ–°æ‰“åŒ… argsï¼Œç¡®ä¿ process_single_image èƒ½æ‹¿åˆ°åˆå¹¶åçš„å‚æ•°
    class MergedArgs:
        pass
    merged_args = MergedArgs()
    merged_args.device = device
    merged_args.use_api = use_api
    merged_args.access_key_id = args.access_key_id
    merged_args.access_key_secret = args.access_key_secret

    # å¦‚æœæœ‰ tqdm å°±ç”¨ï¼Œæ²¡æœ‰å°±æ™®é€šå¾ªç¯
    iterator = tqdm(image_files) if 'tqdm' in sys.modules else image_files

    for img_path in iterator:
        try:
            process_single_image(img_path, output_dir, models, config, merged_args, rag_system)
        except Exception as e:
            # æ‰“å°è¯¦ç»†é”™è¯¯æ ˆï¼Œæ–¹ä¾¿è°ƒè¯•
            import traceback
            print(f"\nâŒ å¤„ç†å¤±è´¥: {img_path}")
            traceback.print_exc()

    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")